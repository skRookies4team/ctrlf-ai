"""
FAQ Draft Service (Phase 18+)

FAQ í›„ë³´ í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FAQ ì´ˆì•ˆì„ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤.
Milvus + LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸/ë‹µë³€/ê·¼ê±° ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

Note:
    RAGFlow í´ë¼ì´ì–¸íŠ¸ëŠ” ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. Milvusë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- MilvusSearchClientë¡œ ë²¡í„° ê²€ìƒ‰ + text ì§ì ‘ ì¡°íšŒ
- NO_DOCS_FOUND ì—ëŸ¬ ì²˜ë¦¬
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê·¼ê±° ê¸°ë°˜ + ì§§ê³  ëª…í™• + ë§ˆí¬ë‹¤ìš´)
- answer_source: TOP_DOCS / MILVUS êµ¬ë¶„
- LOW_RELEVANCE_CONTEXT ì‹¤íŒ¨ ê·œì¹™
- PII ê°•ì°¨ë‹¨: ì…ë ¥/ì¶œë ¥/ì»¨í…ìŠ¤íŠ¸ì— PII ê²€ì¶œ ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨
- í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë¡œê·¸: ai_confidence ê¸°ë°˜ ê²½ê³  ë¡œê·¸
"""

import json
import re
import uuid
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple, Union

from app.clients.llm_client import LLMClient
from app.clients.milvus_client import (
    MilvusSearchClient,
    MilvusSearchError,
    get_milvus_client,
)
from app.core.config import get_settings
from app.core.logging import get_logger
from app.models.faq import (
    FaqDraft,
    FaqDraftGenerateRequest,
    FaqSourceDoc,
)
from app.models.intent import MaskingStage
from app.services.pii_service import PiiService

logger = get_logger(__name__)


# =============================================================================
# Phase 19-AI-2: RAGFlow ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤
# =============================================================================


@dataclass
class RagSearchResult:
    """RAGFlow /v1/chunk/search ì‘ë‹µì˜ ê°œë³„ ê²°ê³¼."""
    title: Optional[str]
    page: Optional[int]
    score: float
    snippet: str

    @classmethod
    def from_chunk(cls, chunk: Dict[str, Any]) -> "RagSearchResult":
        title = chunk.get("document_name") or chunk.get("doc_name") or chunk.get("title")
        page = chunk.get("page_num") or chunk.get("page")
        if page is not None:
            try:
                page = int(page)
            except (TypeError, ValueError):
                page = None
        score = chunk.get("similarity") or chunk.get("score") or 0.0
        try:
            score = float(score)
        except (TypeError, ValueError):
            score = 0.0
        snippet = chunk.get("content") or chunk.get("text") or ""
        if len(snippet) > 500:
            snippet = snippet[:500]
        return cls(title=title, page=page, score=score, snippet=snippet)


# =============================================================================
# Phase 19-AI-3: LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê°œì„ )
# =============================================================================

SYSTEM_PROMPT = """ë„ˆëŠ” ê¸°ì—… ë‚´ë¶€ í•œêµ­ì–´ FAQ ì‘ì„± ë³´ì¡°ìë‹¤.

## í•µì‹¬ ì›ì¹™
1. ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(context_docs) ë²”ìœ„ì—ì„œë§Œ ì‘ì„±í•œë‹¤.
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ì•ŠëŠ”ë‹¤.
3. ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´ statusë¥¼ "LOW_RELEVANCE"ë¡œ ì„¤ì •í•œë‹¤.
4. ëª¨ë“  ì§ˆë¬¸ê³¼ ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.

## ì¶œë ¥ í˜•ì‹
ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì¶œë ¥í•˜ë¼. ê° í•„ë“œëŠ” ë ˆì´ë¸”ê³¼ ì½œë¡ ìœ¼ë¡œ ì‹œì‘í•œë‹¤.

```
status: SUCCESS ë˜ëŠ” LOW_RELEVANCE
question: [canonical_questionì„ ìì—°ìŠ¤ëŸ¬ìš´ FAQ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ë“¬ë˜ ì˜ë¯¸ ë³€ê²½ ê¸ˆì§€]
summary: [1ë¬¸ì¥, ìµœëŒ€ 120ì]
answer_markdown: |
  [ê²°ë¡  1~2ë¬¸ì¥]

  - [í•µì‹¬ ê·œì¹™/ì ˆì°¨ bullet 1]
  - [í•µì‹¬ ê·œì¹™/ì ˆì°¨ bullet 2]
  - [í•µì‹¬ ê·œì¹™/ì ˆì°¨ bullet 3]
  (3~6ê°œ bullet)

  **ì°¸ê³ **
  - [ë¬¸ì„œ íƒ€ì´í‹€ (p.í˜ì´ì§€)]
ai_confidence: [0.00~1.00, ì»¨í…ìŠ¤íŠ¸ ì í•©ë„ê°€ ë†’ì„ìˆ˜ë¡ ë†’ê²Œ]
```

## ì£¼ì˜ì‚¬í•­
- summaryëŠ” ë°˜ë“œì‹œ 120ì ì´ë‚´ë¡œ ì‘ì„±
- answer_markdownì˜ bulletì€ 3~6ê°œ
- ai_confidenceëŠ” ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì˜ ì—°ê´€ì„±ì„ 0.00~1.00ìœ¼ë¡œ í‰ê°€
- ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ìœ¼ë©´: status: LOW_RELEVANCE, ai_confidence: 0.3 ì´í•˜
"""

USER_PROMPT_TEMPLATE = """## ë„ë©”ì¸
{domain}

## ëŒ€í‘œ ì§ˆë¬¸ (canonical_question)
{canonical_question}

## ì‹¤ì œ ì§ì› ì§ˆë¬¸ ì˜ˆì‹œ (sample_questions)
{sample_questions_text}

## ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ (context_docs)
{docs_text}

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ FAQë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.
"""


class FaqGenerationError(Exception):
    """FAQ ìƒì„± ì¤‘ ë°œìƒí•œ ì—ëŸ¬"""
    pass


# Phase 19-AI-2: ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ íƒ€ì…
DocContext = Union[List[FaqSourceDoc], List[RagSearchResult]]


class FaqDraftService:
    """
    FAQ ì´ˆì•ˆ ìƒì„± ì„œë¹„ìŠ¤.

    Milvus ë²¡í„° ê²€ìƒ‰ + LLMì„ ì‚¬ìš©í•˜ì—¬ FAQ ì´ˆì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤.

    Note:
        RAGFlow í´ë¼ì´ì–¸íŠ¸ëŠ” ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. Milvusë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

    ì£¼ìš” ê¸°ëŠ¥:
    - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê·¼ê±° ê¸°ë°˜)
    - answer_source: TOP_DOCS / MILVUS êµ¬ë¶„
    - LOW_RELEVANCE_CONTEXT ì‹¤íŒ¨ ê·œì¹™
    - PII ê°•ì°¨ë‹¨: ì…ë ¥/ì¶œë ¥ì— PII ê²€ì¶œ ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨
    """

    def __init__(
        self,
        milvus_client: Optional[MilvusSearchClient] = None,
        llm_client: Optional[LLMClient] = None,
        pii_service: Optional[PiiService] = None,
    ) -> None:
        """
        FaqDraftService ì´ˆê¸°í™”.

        Note:
            RAGFlow í´ë¼ì´ì–¸íŠ¸ëŠ” ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. Milvusë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        self._milvus_client = milvus_client or get_milvus_client()
        self._llm = llm_client or LLMClient()
        self._pii_service = pii_service or PiiService()

        logger.info("FaqDraftService: Milvus search enabled (RAGFlow removed)")

    async def generate_faq_draft(
        self,
        req: FaqDraftGenerateRequest,
    ) -> FaqDraft:
        """FAQ ì´ˆì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤.

        Phase 19-AI-4: PII ê°•ì°¨ë‹¨
        - ì…ë ¥ì— PII ê²€ì¶œ ì‹œ: PII_DETECTED ì—ëŸ¬
        - ì¶œë ¥ì— PII ê²€ì¶œ ì‹œ: PII_DETECTED_OUTPUT ì—ëŸ¬
        """
        logger.info(
            f"Generating FAQ draft: domain={req.domain}, "
            f"cluster_id={req.cluster_id}, question='{req.canonical_question[:50]}...'"
        )

        # 0. Phase 19-AI-4: ì…ë ¥ PII ê²€ì‚¬
        await self._check_input_pii(req)

        # 1. ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í™•ë³´ (Phase 19-AI-2 + Option 3)
        context_docs, answer_source = await self._get_context_docs(req)
        logger.info(
            f"Got {len(context_docs)} context documents (source: {answer_source})"
        )

        # 2. LLM ë©”ì‹œì§€ êµ¬ì„±
        messages = self._build_llm_messages(req, context_docs, answer_source)

        # 3. LLM í˜¸ì¶œ
        try:
            llm_response = await self._llm.generate_chat_completion(
                messages=messages,
                model=None,
                temperature=0.3,
                max_tokens=2048,
            )
            logger.debug(f"LLM response: {llm_response[:500]}...")

        except Exception as e:
            logger.exception(f"LLM call failed: {e}")
            raise FaqGenerationError(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {str(e)}")

        # 4. ì‘ë‹µ íŒŒì‹± (Phase 19-AI-3: í•„ë“œë³„ í…ìŠ¤íŠ¸ íŒŒì‹±)
        try:
            parsed = self._parse_llm_response(llm_response)
        except Exception as e:
            logger.warning(f"LLM parse failed, retrying once: {e}")

            llm_response = await self._llm.generate_chat_completion(
                messages=messages,
                model=None,
                temperature=0.1,  # ì•ˆì •í™”
                max_tokens=2048,
            )
            parsed = self._parse_llm_response(llm_response)

        # 5. Phase 19-AI-4: ì¶œë ¥ PII ê²€ì‚¬
        await self._check_output_pii(parsed)
        
        # ===============================
        # Phase 48: LLM í•„ë“œ ë³´ì •
        # ===============================
        parsed.setdefault(
            "summary",
            (parsed.get("answer_markdown", "") or "")[:120]
        )

        try:
            parsed["ai_confidence"] = min(
                max(float(parsed.get("ai_confidence", 0.7)), 0.0), 1.0
            )
        except (TypeError, ValueError):
            parsed["ai_confidence"] = 0.7


        # 6. Phase 19-AI-3: LOW_RELEVANCE_CONTEXT ì²´í¬
        # ì„¤ì •ì— ë”°ë¼ ì„ íƒì  ê²€ì¦
        settings = get_settings()
        status = parsed.get("status", "SUCCESS").upper()
        if status == "LOW_RELEVANCE":
            if settings.FAQ_LOW_RELEVANCE_BLOCK:
                # ì°¨ë‹¨ ëª¨ë“œ: ì—ëŸ¬ ë°œìƒ
                logger.warning(f"Low relevance context for query: '{req.canonical_question[:50]}...'")
                raise FaqGenerationError("LOW_RELEVANCE_CONTEXT")
            else:
                # ê²½ê³  ëª¨ë“œ: ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  ê³„ì† ì§„í–‰
                logger.warning(
                    f"Low relevance context detected but continuing "
                    f"(cluster_id={req.cluster_id}, question='{req.canonical_question[:50]}...')"
                )
                # statusë¥¼ SUCCESSë¡œ ê°•ì œ ë³€ê²½í•˜ì—¬ FAQ ìƒì„± í—ˆìš©
                parsed["status"] = "SUCCESS"

        # 7. FaqDraft ìƒì„± (Phase 19-AI-3 + Option 3)
        draft = self._create_faq_draft(req, parsed, context_docs, answer_source)

        # 8. Phase 20-AI-4: í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë¡œê·¸
        self._log_quality_metrics(draft, context_docs, answer_source)

        logger.info(
            f"FAQ draft generated: id={draft.faq_draft_id}, "
            f"source={draft.answer_source}, confidence={draft.ai_confidence}"
        )

        return draft

    # =========================================================================
    # Phase 19-AI-2 + Option 3: ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í™•ë³´
    # =========================================================================

    async def _get_context_docs(
        self,
        req: FaqDraftGenerateRequest,
    ) -> Tuple[DocContext, str]:
        """
        FAQ ìƒì„±ì— ì‚¬ìš©í•  ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.

        ìš°ì„ ìˆœìœ„:
        1. request.top_docsê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        2. MilvusSearchClientë¡œ ë²¡í„° ê²€ìƒ‰

        Note:
            RAGFlow í´ë¼ì´ì–¸íŠ¸ëŠ” ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. Milvusë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            req: FAQ ì´ˆì•ˆ ìƒì„± ìš”ì²­

        Returns:
            Tuple[DocContext, str]: (ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸, ì†ŒìŠ¤ íƒ€ì…)
                ì†ŒìŠ¤ íƒ€ì…: "TOP_DOCS", "MILVUS"

        Raises:
            FaqGenerationError: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° (NO_DOCS_FOUND)
        """
        # 1. top_docsê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if req.top_docs:
            logger.info(f"Using {len(req.top_docs)} provided top_docs")
            return req.top_docs, "TOP_DOCS"

        # 2. Milvus ê²€ìƒ‰
        return await self._search_milvus(req)

    async def _search_milvus(
        self,
        req: FaqDraftGenerateRequest,
    ) -> Tuple[DocContext, str]:
        """
        Milvusì—ì„œ ì§ì ‘ ë²¡í„° ê²€ìƒ‰ + text ì¡°íšŒ.

        Note:
            RAGFlow í´ë¼ì´ì–¸íŠ¸ëŠ” ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. Milvusë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            req: FAQ ì´ˆì•ˆ ìƒì„± ìš”ì²­

        Returns:
            Tuple[DocContext, str]: (ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸, "MILVUS")

        Raises:
            FaqGenerationError: ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ
        """
        logger.info(f"Searching Milvus for: '{req.canonical_question[:50]}...'")

        # ===============================
        # Phase 48: Multi-query Milvus search
        # ===============================
        queries = [req.canonical_question]
        if req.sample_questions:
            queries.extend(req.sample_questions[:2])  # ìµœëŒ€ 2ê°œ

        raw_results: List[dict] = []

        for q in queries:
            try:
                partial = await self._milvus_client.search(
                    query=q,
                    domain=req.domain,
                    top_k=3,
                )
                raw_results.extend(partial)
            except MilvusSearchError as e:
                logger.error(f"Milvus search error (query='{q[:30]}...'): {e}")
                continue

        if not raw_results:
            logger.warning("No documents found in Milvus (multi-query)")
            raise FaqGenerationError("NO_DOCS_FOUND")

        # ===============================
        # dedup + RagSearchResult ë³€í™˜
        # ===============================
        merged: Dict[tuple, RagSearchResult] = {}

        for r in raw_results:
            key = (r.get("doc_id"), r.get("metadata", {}).get("chunk_id"))
            score = float(r.get("score", 0.0))

            candidate = RagSearchResult(
                title=r.get("doc_id", ""),
                page=r.get("metadata", {}).get("chunk_id"),
                score=score,
                snippet=(r.get("content") or "")[:500],
            )

            if key not in merged or merged[key].score < score:
                merged[key] = candidate

        context_docs = sorted(
            merged.values(), key=lambda x: x.score, reverse=True
        )[:5]

        # ===============================
        # ğŸ”¥ ì ìˆ˜ ê¸°ë°˜ ì‚¬ì „ ì°¨ë‹¨ (LLM í˜¸ì¶œ ì „)
        # ===============================
        settings = get_settings()
        min_score = getattr(settings, "FAQ_MIN_MILVUS_SCORE", 0.55)

        top_score = context_docs[0].score if context_docs else 0.0
        if top_score < min_score:
            logger.warning(
                f"LOW_RELEVANCE before LLM: top_score={top_score:.2f} < {min_score}"
            )
            raise FaqGenerationError("LOW_RELEVANCE_CONTEXT")

        # ì»¨í…ìŠ¤íŠ¸ PII ê²€ì‚¬
        await self._check_context_pii(context_docs, req.domain, req.cluster_id)

        return context_docs, "MILVUS"


    # =========================================================================
    # Phase 19-AI-3: LLM ë©”ì‹œì§€ êµ¬ì„±
    # =========================================================================

    def _build_llm_messages(
        self,
        req: FaqDraftGenerateRequest,
        context_docs: DocContext,
        answer_source: str,
    ) -> List[dict]:
        """
        LLM í˜¸ì¶œìš© ë©”ì‹œì§€ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

        Args:
            req: FAQ ì´ˆì•ˆ ìƒì„± ìš”ì²­
            context_docs: ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ (FaqSourceDoc ë˜ëŠ” RagSearchResult)
            answer_source: ì†ŒìŠ¤ íƒ€ì… ("TOP_DOCS", "MILVUS", "RAGFLOW")

        Returns:
            LLM ë©”ì‹œì§€ ëª©ë¡
        """
        # ìƒ˜í”Œ ì§ˆë¬¸ í¬ë§·
        if req.sample_questions:
            sample_questions_text = "\n".join(
                f"- {q}" for q in req.sample_questions[:5]
            )
        else:
            sample_questions_text = "(ì—†ìŒ)"

        # ë¬¸ì„œ ë°œì·Œ í¬ë§·
        docs_text = self._format_docs_for_prompt(context_docs, answer_source)

        # User ë©”ì‹œì§€ ìƒì„±
        user_message = USER_PROMPT_TEMPLATE.format(
            domain=req.domain,
            canonical_question=req.canonical_question,
            sample_questions_text=sample_questions_text,
            docs_text=docs_text,
        )

        return [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_message},
        ]

    def _format_docs_for_prompt(
        self,
        context_docs: DocContext,
        answer_source: str,
    ) -> str:
        """
        ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ í¬ë§·í•©ë‹ˆë‹¤.

        Args:
            context_docs: ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            answer_source: ì†ŒìŠ¤ íƒ€ì… ("TOP_DOCS", "MILVUS", "RAGFLOW")

        Returns:
            í¬ë§·ëœ ë¬¸ì„œ ë¬¸ìì—´
        """
        if not context_docs:
            return "(ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ì—†ìŒ)"

        docs_lines = []

        if answer_source == "TOP_DOCS":
            # FaqSourceDoc í¬ë§·
            for i, doc in enumerate(context_docs[:5], start=1):
                doc_info = f"### ë¬¸ì„œ {i}: {doc.title or 'ì œëª© ì—†ìŒ'}"
                if doc.article_label:
                    doc_info += f" ({doc.article_label})"
                if doc.snippet:
                    doc_info += f"\n{doc.snippet[:500]}"
                docs_lines.append(doc_info)
        else:
            # RagSearchResult í¬ë§· (MILVUS / RAGFLOW)
            for i, doc in enumerate(context_docs[:5], start=1):
                doc_info = f"### ë¬¸ì„œ {i}: {doc.title or 'ì œëª© ì—†ìŒ'}"
                if doc.page is not None:
                    doc_info += f" (chunk #{doc.page})"
                doc_info += f" [ìœ ì‚¬ë„: {doc.score:.2f}]"
                if doc.snippet:
                    doc_info += f"\n{doc.snippet}"
                docs_lines.append(doc_info)

        return "\n\n".join(docs_lines)

    # =========================================================================
    # Phase 19-AI-3: FaqDraft ìƒì„±
    # =========================================================================

    def _create_faq_draft(
        self,
        req: FaqDraftGenerateRequest,
        parsed: dict,
        context_docs: DocContext,
        answer_source: str,
    ) -> FaqDraft:
        """
        íŒŒì‹±ëœ LLM ì‘ë‹µìœ¼ë¡œ FaqDraftë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Phase 19-AI-3 + Option 3 ìŠ¤í™:
        - answer_source: TOP_DOCS, MILVUS, ë˜ëŠ” RAGFLOW
        - top_docs ì‚¬ìš© ì‹œ: source_doc_id ë“± ê·¸ëŒ€ë¡œ ì‚¬ìš©
        - MILVUS/RAGFLOW ê²€ìƒ‰ ì‹œ: source í•„ë“œëŠ” null

        Args:
            req: FAQ ì´ˆì•ˆ ìƒì„± ìš”ì²­
            parsed: íŒŒì‹±ëœ LLM ì‘ë‹µ
            context_docs: ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            answer_source: ì†ŒìŠ¤ íƒ€ì… ("TOP_DOCS", "MILVUS", "RAGFLOW")

        Returns:
            FaqDraft: ìƒì„±ëœ FAQ ì´ˆì•ˆ
        """
        answer_markdown = parsed.get("answer_markdown", "")

        # source í•„ë“œ ê²°ì •
        if answer_source == "TOP_DOCS" and context_docs:
            first_doc = context_docs[0]
            source_doc_id = first_doc.doc_id
            source_doc_version = first_doc.doc_version
            source_article_label = first_doc.article_label
            source_article_path = first_doc.article_path
        else:
            # RAGFlow ê²€ìƒ‰ ê²°ê³¼ëŠ” source ì •ë³´ ì—†ìŒ
            source_doc_id = None
            source_doc_version = None
            source_article_label = None
            source_article_path = None

        # summary ê¸¸ì´ ê²€ì¦ (Phase 19-AI-3: ìµœëŒ€ 120ì)
        summary = parsed.get("summary", "")
        if len(summary) > 120:
            summary = summary[:117] + "..."

        return FaqDraft(
            faq_draft_id=str(uuid.uuid4()),
            cluster_id=req.cluster_id,
            domain=req.domain,
            question=parsed.get("question", req.canonical_question),
            answer_markdown=answer_markdown,
            summary=summary,
            source_doc_id=source_doc_id,
            source_doc_version=source_doc_version,
            source_article_label=source_article_label,
            source_article_path=source_article_path,
            answer_source=answer_source,
            ai_confidence=self._normalize_confidence(parsed.get("ai_confidence")),
            created_at=datetime.now(timezone.utc),
        )

    # =========================================================================
    # Phase 19-AI-3: LLM ì‘ë‹µ íŒŒì‹± (í•„ë“œë³„ í…ìŠ¤íŠ¸ + JSON ì§€ì›)
    # =========================================================================

    def _parse_llm_response(self, llm_response: str) -> dict:
        """
        LLM ì‘ë‹µì„ íŒŒì‹±í•©ë‹ˆë‹¤.

        Phase 19-AI-3: í•„ë“œë³„ í…ìŠ¤íŠ¸ í˜•ì‹ê³¼ JSON í˜•ì‹ ëª¨ë‘ ì§€ì›

        Args:
            llm_response: LLM ì‘ë‹µ í…ìŠ¤íŠ¸

        Returns:
            íŒŒì‹±ëœ ë”•ì…”ë„ˆë¦¬

        Raises:
            FaqGenerationError: íŒŒì‹± ì‹¤íŒ¨ ì‹œ
        """
        # ë°©ë²• 1: í•„ë“œë³„ í…ìŠ¤íŠ¸ í˜•ì‹ íŒŒì‹± ì‹œë„
        parsed = self._parse_field_text_format(llm_response)
        if parsed and parsed.get("question") and parsed.get("answer_markdown"):
            return parsed

        # ë°©ë²• 2: JSON í˜•ì‹ íŒŒì‹± ì‹œë„ (í•˜ìœ„ í˜¸í™˜)
        json_str = self._extract_json_from_response(llm_response)
        if json_str:
            try:
                data = json.loads(json_str)
                if data.get("question") and data.get("answer_markdown"):
                    return data
            except json.JSONDecodeError:
                pass

        # íŒŒì‹± ì‹¤íŒ¨
        raise FaqGenerationError("LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: í•„ë“œë³„ í…ìŠ¤íŠ¸ ë˜ëŠ” JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    def _parse_field_text_format(self, response: str) -> Optional[dict]:
        """
        Phase 19-AI-3: í•„ë“œë³„ í…ìŠ¤íŠ¸ í˜•ì‹ì„ íŒŒì‹±í•©ë‹ˆë‹¤.

        í˜•ì‹:
        status: SUCCESS
        question: ì§ˆë¬¸ ë‚´ìš©
        summary: ìš”ì•½ ë‚´ìš©
        answer_markdown: |
          ë‹µë³€ ë‚´ìš©
        ai_confidence: 0.85

        Args:
            response: LLM ì‘ë‹µ í…ìŠ¤íŠ¸

        Returns:
            íŒŒì‹±ëœ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        result = {}

        # status ì¶”ì¶œ
        status_match = re.search(r'^status:\s*(.+)$', response, re.MULTILINE | re.IGNORECASE)
        if status_match:
            result["status"] = status_match.group(1).strip()

        # question ì¶”ì¶œ
        question_match = re.search(r'^question:\s*(.+)$', response, re.MULTILINE | re.IGNORECASE)
        if question_match:
            result["question"] = question_match.group(1).strip()

        # summary ì¶”ì¶œ
        summary_match = re.search(r'^summary:\s*(.+)$', response, re.MULTILINE | re.IGNORECASE)
        if summary_match:
            result["summary"] = summary_match.group(1).strip()

        # ai_confidence ì¶”ì¶œ
        confidence_match = re.search(r'^ai_confidence:\s*([\d.]+)', response, re.MULTILINE | re.IGNORECASE)
        if confidence_match:
            try:
                result["ai_confidence"] = float(confidence_match.group(1))
            except ValueError:
                pass

        # answer_markdown ì¶”ì¶œ (ë©€í‹°ë¼ì¸)
        # íŒ¨í„´ 1: answer_markdown: | ë¡œ ì‹œì‘í•˜ëŠ” YAML ìŠ¤íƒ€ì¼
        md_match = re.search(
            r'^answer_markdown:\s*\|?\s*\n((?:[ \t]+.+\n?)+)',
            response,
            re.MULTILINE | re.IGNORECASE
        )
        if md_match:
            # ë“¤ì—¬ì“°ê¸° ì œê±°
            lines = md_match.group(1).split('\n')
            # ìµœì†Œ ë“¤ì—¬ì“°ê¸° ì°¾ê¸°
            min_indent = float('inf')
            for line in lines:
                if line.strip():
                    indent = len(line) - len(line.lstrip())
                    min_indent = min(min_indent, indent)
            if min_indent == float('inf'):
                min_indent = 0
            # ë“¤ì—¬ì“°ê¸° ì œê±°í•˜ì—¬ ê²°í•©
            cleaned_lines = []
            for line in lines:
                if len(line) >= min_indent:
                    cleaned_lines.append(line[min_indent:])
                else:
                    cleaned_lines.append(line)
            result["answer_markdown"] = '\n'.join(cleaned_lines).strip()
        else:
            # íŒ¨í„´ 2: answer_markdown: ì´í›„ ë‹¤ìŒ í•„ë“œê¹Œì§€
            md_match2 = re.search(
                r'^answer_markdown:\s*(.+?)(?=^(?:ai_confidence|status|question|summary):|\Z)',
                response,
                re.MULTILINE | re.DOTALL | re.IGNORECASE
            )
            if md_match2:
                result["answer_markdown"] = md_match2.group(1).strip()

        return result if result else None

    def _extract_json_from_response(self, response: str) -> Optional[str]:
        """
        LLM ì‘ë‹µì—ì„œ JSON ë¬¸ìì—´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            response: LLM ì‘ë‹µ í…ìŠ¤íŠ¸

        Returns:
            JSON ë¬¸ìì—´ ë˜ëŠ” None
        """
        # ë°©ë²• 1: ```json ... ``` ë¸”ë¡ì—ì„œ ì¶”ì¶œ
        json_block_pattern = r"```(?:json)?\s*\n?([\s\S]*?)\n?```"
        match = re.search(json_block_pattern, response)
        if match:
            return match.group(1).strip()

        # ë°©ë²• 2: { ... } íŒ¨í„´ì—ì„œ ì¶”ì¶œ
        brace_pattern = r"\{[\s\S]*\}"
        match = re.search(brace_pattern, response)
        if match:
            return match.group(0).strip()

        return None

    # =========================================================================
    # Phase 19-AI-4: PII ê°•ì°¨ë‹¨
    # =========================================================================

    async def _check_input_pii(self, req: FaqDraftGenerateRequest) -> None:
        """ì…ë ¥ ë°ì´í„°ì—ì„œ PIIë¥¼ ê²€ì‚¬í•©ë‹ˆë‹¤.

        ê²€ì‚¬ ëŒ€ìƒ:
        - canonical_question
        - sample_questions (ëª¨ë“  í•­ëª©)
        - top_docs.snippet (ëª¨ë“  í•­ëª©)

        Args:
            req: FAQ ì´ˆì•ˆ ìƒì„± ìš”ì²­

        Raises:
            FaqGenerationError: PII ê²€ì¶œ ì‹œ "PII_DETECTED" ì—ëŸ¬
        """
        # ê²€ì‚¬í•  í…ìŠ¤íŠ¸ ëª©ë¡ ìˆ˜ì§‘
        texts_to_check: List[str] = []

        # 1. canonical_question
        if req.canonical_question:
            texts_to_check.append(req.canonical_question)

        # 2. sample_questions
        if req.sample_questions:
            texts_to_check.extend(req.sample_questions)

        # 3. top_docs.snippet
        if req.top_docs:
            for doc in req.top_docs:
                if doc.snippet:
                    texts_to_check.append(doc.snippet)

        # PII ê²€ì‚¬ ìˆ˜í–‰
        for text in texts_to_check:
            if not text or not text.strip():
                continue

            result = await self._pii_service.detect_and_mask(text, MaskingStage.INPUT)

            if result.has_pii:
                logger.warning(
                    f"PII detected in input: {len(result.tags)} entities found, "
                    f"labels={[tag.label for tag in result.tags]}"
                )
                raise FaqGenerationError("PII_DETECTED")

        logger.debug("Input PII check passed")

    async def _check_output_pii(self, parsed: Dict[str, Any]) -> None:
        """LLM ì¶œë ¥ ë°ì´í„°ì—ì„œ PIIë¥¼ ê²€ì‚¬í•©ë‹ˆë‹¤.

        ê²€ì‚¬ ëŒ€ìƒ:
        - answer_markdown
        - summary

        Args:
            parsed: íŒŒì‹±ëœ LLM ì‘ë‹µ

        Raises:
            FaqGenerationError: PII ê²€ì¶œ ì‹œ "PII_DETECTED_OUTPUT" ì—ëŸ¬
        """
        # ê²€ì‚¬í•  í…ìŠ¤íŠ¸ ëª©ë¡ ìˆ˜ì§‘
        texts_to_check: List[str] = []

        # 1. answer_markdown
        answer_markdown = parsed.get("answer_markdown", "")
        if answer_markdown:
            texts_to_check.append(answer_markdown)

        # 2. summary
        summary = parsed.get("summary", "")
        if summary:
            texts_to_check.append(summary)

        # PII ê²€ì‚¬ ìˆ˜í–‰
        for text in texts_to_check:
            if not text or not text.strip():
                continue

            result = await self._pii_service.detect_and_mask(text, MaskingStage.OUTPUT)

            if result.has_pii:
                logger.warning(
                    f"PII detected in output: {len(result.tags)} entities found, "
                    f"labels={[tag.label for tag in result.tags]}"
                )
                raise FaqGenerationError("PII_DETECTED_OUTPUT")

        logger.debug("Output PII check passed")

    # =========================================================================
    # Phase 20-AI-3: ì»¨í…ìŠ¤íŠ¸ PII ê°•ì°¨ë‹¨
    # =========================================================================

    async def _check_context_pii(
        self,
        context_docs: List[RagSearchResult],
        domain: str,
        cluster_id: str,
    ) -> None:
        """RAGFlow ê²€ìƒ‰ ê²°ê³¼ snippetì—ì„œ PIIë¥¼ ê²€ì‚¬í•©ë‹ˆë‹¤.

        Phase 20-AI-3: RAGFlow ìŠ¤ë‹ˆí«ì— PIIê°€ í¬í•¨ë˜ë©´ ê°•ì°¨ë‹¨.
        - ê²€ì‚¬ ëŒ€ìƒ: ê° context_docsì˜ snippet
        - PII ë°œê²¬ ì‹œ: PII_DETECTED_CONTEXT ì—ëŸ¬
        - ë¡œê·¸ì— domain, cluster_id, ë¬¸ì„œ ì œëª© ì¼ë¶€ë¥¼ ë‚¨ê¸°ë˜ PII ì›ë¬¸ì€ ë¡œê·¸ ê¸ˆì§€

        Args:
            context_docs: RAGFlow ê²€ìƒ‰ ê²°ê³¼ (RagSearchResult ë¦¬ìŠ¤íŠ¸)
            domain: FAQ ë„ë©”ì¸
            cluster_id: FAQ í´ëŸ¬ìŠ¤í„° ID

        Raises:
            FaqGenerationError: PII ê²€ì¶œ ì‹œ "PII_DETECTED_CONTEXT" ì—ëŸ¬
        """
        for i, doc in enumerate(context_docs):
            if not doc.snippet or not doc.snippet.strip():
                continue

            result = await self._pii_service.detect_and_mask(
                doc.snippet, MaskingStage.INPUT
            )

            if result.has_pii:
                # PII ì›ë¬¸ì€ ë¡œê·¸í•˜ì§€ ì•Šê³ , ë¬¸ì„œ ì œëª©/ì¸ë±ìŠ¤ë§Œ ë¡œê·¸
                doc_title_safe = (doc.title or "untitled")[:30]
                logger.warning(
                    f"PII detected in RAGFlow context: "
                    f"domain={domain}, cluster_id={cluster_id}, "
                    f"doc_index={i}, doc_title='{doc_title_safe}...', "
                    f"pii_count={len(result.tags)}, pii_labels={[tag.label for tag in result.tags]}",
                    extra={
                        "event": "pii_detected_context",
                        "domain": domain,
                        "cluster_id": cluster_id,
                        "doc_index": i,
                    }
                )
                raise FaqGenerationError("PII_DETECTED_CONTEXT")

        logger.debug(
            f"Context PII check passed: {len(context_docs)} docs checked"
        )

    # =========================================================================
    # Phase 20-AI-4: í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë¡œê·¸
    # =========================================================================

    def _log_quality_metrics(
        self,
        draft: FaqDraft,
        context_docs: DocContext,
        answer_source: str,
    ) -> None:
        """FAQ ìƒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.

        Phase 20-AI-4: ai_confidence ê¸°ë°˜ ê²½ê³  ë¡œê·¸
        - status=SUCCESSì¸ë° ai_confidence < threshold -> WARN ë¡œê·¸
        - êµ¬ì¡°í™” ë¡œê·¸ë¡œ ëŒ€ì‹œë³´ë“œ ì§‘ê³„ ì§€ì›

        Args:
            draft: ìƒì„±ëœ FAQ ì´ˆì•ˆ
            context_docs: ì‚¬ìš©ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            answer_source: ì†ŒìŠ¤ íƒ€ì… ("TOP_DOCS", "MILVUS", "RAGFLOW")
        """
        settings = get_settings()
        threshold = settings.FAQ_CONFIDENCE_WARN_THRESHOLD

        # ê²€ìƒ‰ top score ì¶”ì¶œ (MILVUS / RAGFLOW)
        search_top_score = None
        if answer_source in ("MILVUS", "RAGFLOW") and context_docs:
            first_doc = context_docs[0]
            if hasattr(first_doc, "score"):
                search_top_score = first_doc.score

        log_extra = {
            "event": "faq_quality",
            "cluster_id": draft.cluster_id,
            "domain": draft.domain,
            "answer_source": draft.answer_source,
            "ai_confidence": draft.ai_confidence,
            "status": "SUCCESS",
            "error_message": None,
            "search_top_score": search_top_score,
        }

        # ai_confidenceê°€ ë‚®ìœ¼ë©´ ê²½ê³  ë¡œê·¸
        if draft.ai_confidence is not None and draft.ai_confidence < threshold:
            logger.warning(
                f"Low confidence FAQ generated: "
                f"cluster_id={draft.cluster_id}, domain={draft.domain}, "
                f"ai_confidence={draft.ai_confidence:.2f} (threshold={threshold}), "
                f"answer_source={draft.answer_source}",
                extra=log_extra,
            )
        else:
            logger.info(
                f"FAQ quality metrics: "
                f"cluster_id={draft.cluster_id}, ai_confidence={draft.ai_confidence}, "
                f"answer_source={draft.answer_source}",
                extra=log_extra,
            )

    def _log_failed_quality_metrics(
        self,
        req: FaqDraftGenerateRequest,
        error_message: str,
    ) -> None:
        """FAQ ìƒì„± ì‹¤íŒ¨ ì‹œ í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.

        Phase 20-AI-4: ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ëª¨ë‹ˆí„°ë§
        - LOW_RELEVANCE_CONTEXT, NO_DOCS_FOUND ë“± ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¶”ì 

        Args:
            req: FAQ ì´ˆì•ˆ ìƒì„± ìš”ì²­
            error_message: ì—ëŸ¬ ë©”ì‹œì§€
        """
        log_extra = {
            "event": "faq_quality",
            "cluster_id": req.cluster_id,
            "domain": req.domain,
            "answer_source": None,
            "ai_confidence": None,
            "status": "FAILED",
            "error_message": error_message,
            "ragflow_top_score": None,
        }

        # íŠ¹ì • ì—ëŸ¬ ì½”ë“œì— ëŒ€í•´ ê²½ê³  ë ˆë²¨ ë¡œê·¸
        if error_message in ("LOW_RELEVANCE_CONTEXT", "NO_DOCS_FOUND"):
            logger.warning(
                f"FAQ generation quality issue: "
                f"cluster_id={req.cluster_id}, domain={req.domain}, "
                f"error={error_message}",
                extra=log_extra,
            )
        else:
            logger.info(
                f"FAQ generation failed: "
                f"cluster_id={req.cluster_id}, domain={req.domain}, "
                f"error={error_message}",
                extra=log_extra,
            )

    # =========================================================================
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    # =========================================================================

    def _normalize_answer_source(
        self, source: Optional[str]
    ) -> str:
        """answer_sourceë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤. (í•˜ìœ„ í˜¸í™˜ìš©)"""
        valid_sources = {"TOP_DOCS", "MILVUS", "AI_RAG", "LOG_REUSE", "MIXED"}
        if source and source.upper() in valid_sources:
            return source.upper()
        return "MILVUS"

    def _normalize_confidence(
        self, confidence: Optional[float]
    ) -> Optional[float]:
        """ai_confidenceë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        if confidence is None:
            return None
        try:
            val = float(confidence)
            return max(0.0, min(1.0, val))
        except (TypeError, ValueError):
            return None
